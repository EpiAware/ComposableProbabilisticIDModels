---
title: "Composable probabilistic models can lower barriers to rigorous infectious disease modelling"
title-slide-attributes:
  data-notes: ""
author: "Sam Abbott, Samuel P. C. Brand, Sandra Montes-Olivas, Hong Ge, <br> Kaitlyn E. Johnson, Simon D. W. Frost, Anne Cori, Sebastian Funk"
date: today
---

## {background-color="#e6a817"}

::: {.callout-note}
## Disclaimer

 This slide deck was generated by
[@seabbs-bot](https://github.com/seabbs-bot).
:::

## TL;DR

::: {.callout-note}
## Read the paper

epiaware.org/ComposableProbabilisticIDModels
:::

- Modelling evidence must be timely, rigorous, and
  collaborative but current approaches force a trade-off
- Chaining models offers flexibility but loses information
  and introduces bias; joint models are rigorous but
  monolithic and not reusable
- Composable models can be developed and validated as parts
  then composed for joint analysis, supporting a rigorous
  modelling workflow
- A proof of concept in Julia/R replicates three published
  analyses by composing shared and novel components
- Work remains but investment in adaptable modelling
  infrastructure that can incorporate diverse data to
  provide timely evidence for policy is critical

## Talk plan

- Background
- Proposed requirements
- Our approach
- Case studies
- Wrap up

# Background

## The gap: timely, rigorous, and collaborative evidence

:::: {.columns}
::: {.column width="50%"}
- Modelling evidence must be timely, rigorous,
  and collaborative
- Current approaches struggle to be all three
:::
::: {.column width="50%"}
*Whitty 2015 paper screenshot*
:::
::::

## The gap: COVID-19 showed what is possible

:::: {.columns}
::: {.column width="50%"}
- COVID-19 models integrated cases, prevalence,
  severity, and hospital occupancy for policy
- Joint models provided evidence that supported
  policy decisions
- But required expertise across multiple domains
  and long development timelines
:::
::: {.column width="50%"}
*Birrell et al. COVID-19 model screenshot*
:::
::::

## The gap: flexibility for new contexts

:::: {.columns}
::: {.column width="50%"}
- For mpox, these models lacked the flexibility
  to adapt; new single-source models were built
  instead
- Different contact structure and behaviour
  needed explicit representation
:::
::: {.column width="50%"}
*Endo et al. mpox paper screenshot*
:::
::::

## The gap: integrating diverse data

:::: {.columns}
::: {.column width="50%"}
- Cross-sectional viral loads shown informative
  for epidemic dynamics
- Not incorporated into most policy models even
  where routinely available
- Existing frameworks too rigid to add new data
  sources without rebuilding
:::
::: {.column width="50%"}
*Hay et al. viral loads paper screenshot*
:::
::::

## The gap: multi-model efforts

:::: {.columns}
::: {.column width="50%"}
- Multi-model efforts synthesise expertise by
  combining forecasts from multiple teams
- Improves predictive accuracy but teams rarely
  collaborate on model components
- Common components would enable attribution of
  differences to assumptions rather than
  implementation
:::
::: {.column width="50%"}
*Multi-model hub paper screenshot*
:::
::::

## Analysing data separately: the data

:::: {.columns}
::: {.column width="50%"}
- ONS CIS: 4M+ swabs, 150K households,
  拢500M+, April 2020 to March 2023
- External modellers accessed only summarised
  prevalence, not underlying observations
- Identifiability concerns drove this restriction
:::
::: {.column width="50%"}
*ONS prevalence estimates screenshot*
:::
::::

## Analysing data separately: the cascade

:::: {.columns}
::: {.column width="50%"}
- Prevalence to incidence to Rt to policy
  analyses
- Estimates used as inputs to subsequent
  analyses at each stage
- Uncertainty from earlier estimates
  approximated at each step
- Modelling assumptions inherited without
  the ability to evaluate their impact
:::
::: {.column width="50%"}
*Incidence and Rt estimates screenshot*
:::
::::

## Analysing data separately: the impact

:::: {.columns}
::: {.column width="50%"}
- Cascade fed into policy-relevant analyses
  of variant transmissibility and severity
- No way to evaluate the impact of inherited
  assumptions on downstream conclusions
- Joint modelling of CIS data was not feasible
  within policy timelines (or at least was not
  done)
:::
::: {.column width="50%"}
*Policy paper screenshot*
:::
::::

## Analysing all data together

- Joint modelling avoids cascading bias and
  provides rigorous statistical inference
- But models are difficult to develop within
  policy-relevant timelines
- Require expertise across multiple domains and
  careful specification of component interactions

## Joint modelling: the evidence

:::: {.columns}
::: {.column width="50%"}
- Lison et al. replicated
  [Gunther et al.](https://doi.org/10.1002/bimj.202000112)
  for date imputation, nowcasting, and Rt
  estimation in Bavaria
- Found biases at each step: lack of epidemic
  phase in date imputation and count model
  assumptions in nowcasting led to biased Rt
- Joint modelling of all steps together
  avoided these cascading biases

[DOI: 10.1371/journal.pcbi.1012021](https://doi.org/10.1371/journal.pcbi.1012021)
:::
::: {.column width="50%"}
*Lison et al. 2024 paper screenshot*
:::
::::

## PPLs reduce effort but not reuse

:::: {.columns}
::: {.column width="50%"}
- PPLs let analysts specify models in
  high-level code rather than raw maths
- Stan, LibBi, pomp, monty all reduce
  development effort for joint models
- But no straightforward way to share or
  reuse model components between packages
- Fragmentation persists despite shared
  goals across modelling teams
:::
::: {.column width="50%"}
*Stan splash page screenshot*
:::
::::

## The Rt estimation ecosystem

- At least seven Stan packages estimate Rt using
  renewal approaches; none share components
- Some packages share authors yet still duplicate
  implementations
- Wastewater Rt tools built by non-wastewater
  experts, share no components, unclear which
  choices matter
- Unit of reuse is the whole model, not the
  underlying epidemiological concepts

## Rt estimation research

- Many papers propose incremental
  advances to Rt estimation and related methods
- Often not tested against current best practice
  and built from scratch
- Each typically represents a year or more of
  PhD/postdoc work for incremental features
- Features like interaction matrices, day-of-week
  effects, or latent process changes
- Much of this effort is duplicated across groups
  with shared goals
- Little research directly uses the tools
  developed by other groups
- Difficult to communicate across groups as
  even shared language is lacking (e.g. how
  censoring, truncation, and epidemic phase
  bias are defined in delays research)

## What happens when you try to be modular?

:::: {.columns}
::: {.column width="50%"}
- **What is it?** Modular Bayesian framework
  with multi-strata joint modelling of missing
  dates, nowcasting right-truncated delays,
  ascertainment, latent infection modelling,
  and Rt estimation with a regression interface
  for each component
- **Limitations:**
  - Cannot easily be separated into reusable
    parts
  - Difficult for users to contribute to or
    adapt beyond supported functionality
  - Limited adoption; large API and
    documentation for users to get to grips with
  - Remains limited in scope despite ambition
:::
::: {.column width="50%"}
*epinowcast screenshot*
:::
::::

## This pattern repeats across domains

::: {.callout-tip}
## ケ

Each subdomain rebuilds the same infrastructure
from scratch, just with different names.
And they do it again and again.
:::

- Delay estimation
- Nowcasting
- Viral load modelling
- Severity estimation
- Forecasting and scenario modelling

## A workflow for infectious disease modelling

:::: {.columns}
::: {.column width="50%"}
- A systematic workflow guides iterative
  joint model development and validation
- Makes it easier to develop principled joint
  models in a structured, repeatable way
- Builds on established Bayesian workflows
  but addresses domain-specific challenges
- Provides common language for discussing
  data source properties and trade-offs

[github.com/seabbs/a-workflow-for-infectious-disease-modelling](https://github.com/seabbs/a-workflow-for-infectious-disease-modelling)
:::
::: {.column width="50%"}
*Working draft paper screenshot*
:::
::::

## The workflow

:::: {.columns}
::: {.column width="50%"}
- Guides modellers from research questions
  through model specification to reporting
- Modular structure encourages considering
  each submodel in isolation first
- A single specification is useful for both
  simulation and inference
:::
::: {.column width="50%"}
*Workflow schematic figure*
:::
::::

## Following the workflow exposes gaps

- How to build model components and combine
  them into joint models
- Propagating uncertainty correctly across
  components
- Diagnosing failures at interfaces between
  model components
- Detecting and resolving conflicts between
  data sources
- Hybrid inference methods that combine
  different algorithms for different model
  components
- Staged inference for time-sensitive
  settings
- Real-time updating as new data arrives

## Can composable modelling help?

::: {.callout-note}
## What is composable modelling?

Components can be reused across contexts and
combined in different configurations whilst
maintaining statistical rigour.
:::

- Individual components can be inspected and
  validated in isolation then combined into
  joint models that properly propagate uncertainty
- Domain experts contribute specialised parts
  without understanding the whole system
- Adding data sources requires only a new
  submodel, not rebuilding the entire model
- DSLs in Julia have enabled composable models
  in other fields
  - [SpeedyWeather.jl](https://doi.org/10.21105/joss.06323)
    for weather modelling
  - [HydroModels.jl](https://doi.org/10.5281/zenodo.15549719)
    for hydrological modelling

# Proposed requirements

## Requirements for a composable framework

::: {.callout-warning}
These requirements were developed by the authors
and have not yet received broader community input.
:::

- Based on outbreak modelling challenges,
  experience, workflow, and Bayesian best
  practices
  - **Uncertainty & inference**: propagating
    uncertainty through complex models
  - **Model structure**: separating and
    composing distinct model components
  - **Population heterogeneity**: handling
    stratification across groups
  - **Accessibility & adoption**: lowering
    barriers for diverse users
  - **Interoperability**: ensuring components
    work together and with external tools
  - **Workflow**: supporting rapid, validated
    model development

## Summary of proposed requirements

*Requirements table screenshot*

## Requirements: uncertainty & inference

::: {.callout-note}
Decisions under incomplete knowledge require
models that propagate uncertainty faithfully.
:::

- Probabilistic model specification that
  accounts for incomplete knowledge
- Joint modelling and staged inference that
  avoids cascading biases from chaining models
- Automatic differentiation for efficient
  fitting with modern inference methods
- Switchable inference backends; no single
  approach works universally

## Requirements: model structure

::: {.callout-note}
Infection, latent, and observation processes
require distinct expertise and separate reasoning.
:::

- Clear separation between model components
  by process type
- Easy nesting of models within models;
  new data sources should not require
  modifying entire models
- Support for multiple modelling paradigms
  including compartmental, agent-based,
  and network models

## Requirements: population heterogeneity

::: {.callout-note}
Diseases affect populations differently across
age, location, and risk group.
:::

- Arbitrary stratification for all model
  components
- Automatic management of multi-group
  structure; manual bookkeeping is error-prone
- Programmatic generation and modification
  of model components
- Partial pooling to share information across
  groups with sparse data

## Requirements: accessibility & adoption

::: {.callout-note}
Lowering barriers enables diverse users and
incremental uptake of compositional methods.
:::

- Clear, concise modelling language for
  users without programming expertise
- Components contain both structure and
  prior distributions to encode domain
  expertise
- Support for incremental adoption;
  complete rebuilds often difficult to justify
- Components functional as standalone
  tools outside the framework

## Requirements: interoperability

::: {.callout-note}
Components must work together and integrate
with tools from other domains.
:::

- Standardised interfaces between components
  for correct uncertainty propagation
- Data-agnostic model definitions that
  generalise across datasets, prior predictive
  checks, and forecasting
- Interoperability with other scientific
  computing ecosystems (e.g. ODEs, spatial
  models, machine learning)

## Requirements: modelling workflow

::: {.callout-note}
Outbreak response demands rapid iteration and
validated models on decision-relevant timelines.
:::

- A single specification is useful for simulation
  and inference
- Rapid model iteration; easy to adapt
  as evidence and requirements change
- Easy validation and exploration of
  model components
- Language optimised for LLM-assisted
  construction with automated validation

# Our approach {subtitle="A front-end DSL for epidemiology with a back-end probabilistic programming language"}

## Our approach

### Domain-specific language for epidemiology

### Probabilistic programming language as modelling backend

## Why a domain-specific language?

- A PPL is a generic modelling form of DSL;
  why not have a domain-specific one?
- A DSL lets domain experts specify models
  using epidemiological concepts rather than
  programming constructs
- Epidemiological models share common structure:
  infection processes, latent dynamics,
  observation models
- Components encode domain expertise including
  prior distributions, making it transferable
  between models and teams
- Proven in other fields: SpeedyWeather.jl for
  weather, HydroModels.jl for hydrology

## Domain-specific language

:::: {.columns}
::: {.column width="50%"}
::: {.callout-note}
## Implemented in Julia

Julia's type system and multiple dispatch
enable metaprogramming and domain-specific
languages.
:::

- Interfaces define contracts that model
  components must follow, not how they
  implement them
- Complex models built by nesting simpler
  components like building blocks
- Three component families: infection
  processes, latent processes, and
  observation models
- Components compose across families,
  e.g. an observation model can contain
  a latent process for time-varying
  ascertainment
- Model definitions are data-agnostic and
  reusable across different datasets
- EpiProblem assembles components into a
  complete model ready for inference
- Cross-ecosystem access via EpiAwareR,
  an R interface using JuliaCall
:::
::: {.column width="50%"}
```julia
struct AR{
    D <: Sampleable,
    I <: Sampleable,
    P <: Int,
    E <: AbstractTuringLatentModel
  } <: AbstractTuringLatentModel
    damp_prior::D
    init_prior::I
    p::P
    系_t::E

    function AR(damp_prior, init_prior,
            p, 系_t)
      @assert p == length(damp_prior)
      new{...}(damp_prior, init_prior,
               p, 系_t)
    end
end

```
:::
::::

## Composable applications

:::: {.columns}
::: {.column width="50%"}
- Incubation period model reused in all
  four applications
- Latent infection model shared across
  three applications
- Case observation and viral kinetics
  models each shared between two
- Each component independently validated
  then composed into joint models
- Adding a new application means defining
  only the novel parts
:::
::: {.column width="50%"}
![](../../figures/fig-composable.png)
:::
::::

## Why Turing.jl as the modelling backend?

- Requirements need probabilistic reasoning,
  so a probabilistic programming language (PPL)
- As far as we are aware, only Julia PPLs
  provide the metaprogramming needed for
  domain-specific abstractions with standardised
  interfaces, arbitrary stratification, and
  programming over model structure
- Turing.jl has mature submodel support for
  nesting models within models and extensive
  inference algorithm choice
- Light abstraction layer on top of the wider
  Julia ecosystem rather than a walled garden
- SciML ecosystem for ODEs, neural networks,
  and other scientific computing tools

## Turing.jl as a modelling backend

:::: {.columns}
::: {.column width="50%"}
- generate functions translate DSL
  components into Turing.jl models
- Produces code via the @model macro;
  dual purpose for simulation or inference
- @submodel macro enables nesting,
  so components recurse through
  arbitrary depth
- Many computational components are
  backend-agnostic and portable to other
  frameworks
:::
::: {.column width="50%"}
```julia
@model function generate_latent(
    latent_model::AR, n)
  p = latent_model.p
  ar_init ~ latent_model.init_prior
  damp_AR ~ latent_model.damp_prior
  @submodel 系_t = generate_latent(
    latent_model.系_t, n - p)
  ar = accumulate_scan(
    ARStep(damp_AR), ar_init, 系_t)
  return ar
end

```
:::
::::

# Case studies

## ARIMA worked example

:::: {.columns}
::: {.column width="50%"}
- Builds an ARIMA(2,1,1) process from
  reusable components
- Composes AR, MA, and differencing as
  separate, swappable parts
- Demonstrates fitting to synthetic data
  with a Poisson observation model
- Components are reused across all
  subsequent case studies
:::
::: {.column width="50%"}
$$Z_t = \rho_1 Z_{t-1} + \rho_2 Z_{t-2} + \epsilon_t$$

$$Z_t = \epsilon_t + \theta \epsilon_{t-1}$$

$$\Delta Z_t = Z_t - Z_{t-1}$$

Combined as ARIMA(2,1,1):

$$\Delta Z_t = \rho_1 \Delta Z_{t-1} + \rho_2 \Delta Z_{t-2} + \epsilon_t + \theta \epsilon_{t-1}$$
:::
::::

## Autoregressive example: DSL code

:::: {.columns}
::: {.column width="60%"}
```julia
ar2 = AR(;
  damp_priors = [
    truncated(Normal(0.2, 0.2), 0, 1),
    truncated(Normal(0.1, 0.05), 0, 1)],
  init_priors = [Normal(0, 0.2),
                 Normal(0, 0.2)],
  系_t = HierarchicalNormal(
    std_prior = HalfNormal(0.1)))

ma1 = MA(;
  胃_priors = [truncated(
    Normal(0.0, 0.2), -1, 1)],
  系_t = HierarchicalNormal(
    std_prior = HalfNormal(0.1)))

arma21 = @set ar2.系_t = ma1
arima211 = DiffLatentModel(
  arma21, Normal(0, 0.2); d = 1)

```
:::
::: {.column width="40%"}
![](../../figures/fig-case-studies.png)
:::
::::

## Autoregressive example: results

:::: {.columns}
::: {.column width="50%"}
- Posterior distributions recover the
  simulated parameter values
- Prior predictive samples show expected
  behaviour for each component
- Same components reused in all subsequent
  case studies
:::
::: {.column width="50%"}
![](../../figures/fig-arima.png)
:::
::::

## Mishra et al. renewal model

::: {.callout-note}
Mishra et al. (2020).
*On the derivation of the renewal equation
from an age-dependent branching process.*
[DOI: 10.48550/arXiv.2006.16487](https://doi.org/10.48550/arXiv.2006.16487)
:::

- Estimates time-varying reproduction
  numbers from case data
- Combines the renewal equation with a
  negative binomial observation model
- Applied to COVID-19 cases in South Korea
  (January to July 2020)
- Our replication reuses the AR(2) component
  from the ARIMA example

## Renewal model (Mishra et al.): model

:::: {.columns}
::: {.column width="50%"}
- Reuses `AR` from ARIMA example;
  `@set` updates the damping prior for
  this epidemiological context
- `Renewal` struct computes expected
  infections via the renewal equation
  using a discretised serial interval
- `NegativeBinomialError` links expected
  infections to observed cases with
  overdispersion
- `EpiProblem` assembles all three
  components into a complete model
- Each component generated and checked
  in isolation before composition
- Same `generate_epiaware` call produces
  a `Turing.jl` model for inference
:::
::: {.column width="50%"}
![](../../figures/fig-case-studies.png)
:::
::::

## Renewal model (Mishra et al.): results

:::: {.columns}
::: {.column width="50%"}
- Prior predictive checks for each
  component in isolation
- Recovers R_t peaked at ~10 then rapidly
  dropped below 1 in early March 2020
- Posterior predictive distribution captures
  observed case dynamics
:::
::: {.column width="50%"}
![](../../figures/fig-mishra.png)
:::
::::

## EpiNow2 nowcasting

::: {.callout-note}
Abbott et al. (2020).
*EpiNow2: Estimate Real-Time Case Counts
and Time-Varying Epidemiological Parameters.*
[DOI: 10.12688/wellcomeopenres.16006.2](https://doi.org/10.12688/wellcomeopenres.16006.2)
:::

- Real-time estimation of case counts and
  epidemiological parameters
- Accounts for reporting delays,
  right-truncation, and day-of-week effects
- Applied to daily COVID-19 cases
- Reuses the full ARIMA(2,1,1) and negative
  binomial components

## Real-time nowcasting (EpiNow2): model

:::: {.columns}
::: {.column width="50%"}
- Reuses ARIMA(2,1,1) and `negbin`
  from case study 1 directly
- `broadcast_weekly` wraps ARIMA to
  produce piecewise constant weekly
  R_t values
- `ascertainment_dayofweek` adds
  day-of-week reporting effects via
  softmax transformation
- Two `LatentDelay` wrappers compose
  incubation and reporting delays
  sequentially by nesting structs
- Layered composition: `negbin` wrapped
  by day-of-week, then by incubation
  delay, then by reporting delay
- `EpiProblem` assembles the same way
  as Mishra et al. with richer
  observation model
:::
::: {.column width="50%"}
![](../../figures/fig-case-studies.png)
:::
::::

## Real-time nowcasting (EpiNow2): results

:::: {.columns}
::: {.column width="50%"}
- Widely used real-time tool replicated
  through composition of DSL components
  (took 3 hours to replicate)
- Also replicated via `EpiAwareR` R
  interface
- Posterior accounts for reporting delays
  whilst recovering R_t dynamics
:::
::: {.column width="50%"}
![](../../figures/fig-epinow2.png)
:::
::::

## Chatzilena et al. ODE model

::: {.callout-note}
Chatzilena et al. (2019).
*Contemporary statistical inference for
infectious disease models using Stan.*
[DOI: 10.1016/j.epidem.2019.100363](https://doi.org/10.1016/j.epidem.2019.100363)
:::

- Bayesian inference for a compartmental
  SIR model using ODE solvers
- Compares deterministic and stochastic
  observation model formulations
- Fitted to an influenza outbreak in an
  English boarding school (763 students)
- Introduces the SIR ODE as an alternative
  infection process to the renewal model

## ODE model (influenza): model

:::: {.columns}
::: {.column width="50%"}
- `ODEProcess` replaces `Renewal` as the
  infection process; SIR dynamics via
  the SciML ODE ecosystem
- `PoissonError` for the deterministic
  observation model
- Stochastic variant adds `Ascertainment`
  wrapping `PoissonError` with an AR(1)
  latent process for time-varying
  reporting
- `@set` swaps the observation model to
  create the stochastic variant from
  the deterministic one in one line
- Reuses `AR` struct from earlier case
  studies for observation dynamics
- `EpiProblem` composes identically;
  only the submodel types differ
:::
::: {.column width="50%"}
![](../../figures/fig-case-studies.png)
:::
::::

## ODE model (influenza): results

:::: {.columns}
::: {.column width="50%"}
- Both models recover R_0 of approx. 4,
  consistent with Chatzilena et al.
- Stochastic model better calibrated;
  deterministic has points outside 95% CIs
- Demonstrates support for multiple
  modelling paradigms from the same
  components
:::
::: {.column width="50%"}
![](../../figures/fig-sir.png)
:::
::::

# Wrap up

## Summary

- Composable models address the tension between
  statistical rigour and flexibility for
  collaborative, timely evidence
- Modular design enables faster, less error prone
  model development and component reuse
- Domain experts contribute specialised components
  without needing to implement other components
- Common components enable attribution of results
  to modelling assumptions, not implementation
- Component structure is well suited for LLM
  assisted model construction

## Alternative approaches

- Category theory / AlgebraicJulia provide formal
  compositional guarantees but limited support
  for probabilistic modelling
- Stan optimised for complete models; Gen.jl
  lower-level; NumPyro/JAX promising but
  barriers for epidemiological modellers
  (and, imo, ugly and not elegant)
- Declarative graph-based PPLs (JuliaBUGS,
  RxInfer) trade flexibility for structure
- Symbolic-numeric frameworks (ModelingToolkit.jl)
  offer automated optimisation
- Agent-based approaches (Starsim, EpiABM) show
  promise but calibration remains challenging

## Future work

- Gather community feedback on requirements;
  expand component library across scales
  and data types
- Integrate composable approach with proposed
  modelling workflow for structured iterative
  development and validation
- Explore other composability methods such as
  AlgebraicJulia for formal compositional
  guarantees and symbolic-numeric approaches
- Further develop modular Bayesian inference
  methods including Markov melding and cut
  likelihoods
- Improve Turing.jl composability: submodel
  handling, conditioning, and post-processing
- Design surveillance programmes with composable
  frameworks in mind for reuse from data
  collection through to policy analyses

## What I want from you

- **Counter examples** where current practice IS
  producing timely, rigorous, and collaborative
  evidence
- Ideas for solutions that are **not** composable
  models
- **Feedback** on the proposed requirements for
  composable modelling
- Ideas for **other ways** these could be
  implemented (and let's test them)
- **Testing** other approaches in Julia, e.g.
  AlgebraicJulia for formal guarantees
- The infectious disease ecosystem for Julia is
  fairly nascent; for composability to work it
  needs an ecosystem to compose. If you are
  thinking of developing software, **do it in
  Julia**. Speak to me.
- **Thoughts** on composability in general or on
  the modelling workflow

## TL;DR

::: {.callout-note}
## Read the paper

epiaware.org/ComposableProbabilisticIDModels
:::

- Modelling evidence must be timely, rigorous, and
  collaborative but current approaches force a trade-off
- Chaining models offers flexibility but loses information
  and introduces bias; joint models are rigorous but
  monolithic and not reusable
- Composable models can be developed and validated as parts
  then composed for joint analysis, supporting a rigorous
  modelling workflow
- A proof of concept in Julia/R replicates three published
  analyses by composing shared and novel components
- Work remains but investment in adaptable modelling
  infrastructure that can incorporate diverse data to
  provide timely evidence for policy is critical
