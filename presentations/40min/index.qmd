---
title: "Composable probabilistic models can lower barriers to rigorous infectious disease modelling"
title-slide-attributes:
  data-notes: ""
author: "**Sam Abbott**, Samuel P. C. Brand, Sandra Montes-Olivas, Hong Ge, <br> Kaitlyn E. Johnson, Simon D. W. Frost, Anne Cori, Sebastian Funk"
date: today
---

## TL;DR

- Modelling evidence must be timely, rigorous, and
  collaborative but current approaches force a trade-off
- Chaining models offers flexibility but loses information
  and introduces bias; joint models are rigorous but
  monolithic and not reusable
- Composable models can be developed and validated as parts
  then composed for joint analysis, supporting a rigorous
  modelling workflow
- A proof of concept in Julia/R replicates three published
  analyses by composing shared and novel components
- Work remains but investment in adaptable modelling
  infrastructure that can incorporate diverse data to
  provide timely evidence for policy is critical

::: {.callout-note}
## Links

- epiaware.org/ComposableProbabilisticIDModels
:::

## Talk plan

- Background
- Proposed requirements
- Our approach
- Case studies
- Wrap up

# Background

## The gap

- Modelling evidence must be timely, rigorous,
  and collaborative; current approaches struggle
  to be all three
- COVID-19 models integrated cases, prevalence,
  severity, and hospital occupancy for policy
- For mpox, these models lacked the flexibility
  to adapt; new single-source models were built
  instead
- Cross-sectional viral loads shown informative
  but not incorporated even where routinely
  available
- Multi-model efforts rarely collaborate on
  components despite shared goals

## Analysing data separately: the data

:::: {.columns}
::: {.column width="50%"}
- ONS CIS: 4M+ swabs, 150K households,
  Â£500M+, April 2020 to March 2023
- External modellers accessed only summarised
  prevalence, not underlying observations
- Identifiability concerns drove this restriction
:::
::: {.column width="50%"}
*ONS prevalence estimates screenshot*
:::
::::

## Analysing data separately: the cascade

:::: {.columns}
::: {.column width="50%"}
- Prevalence to incidence to Rt to policy
  analyses
- Uncertainty approximated and assumptions
  inherited at each stage without ability to
  evaluate their impact
- May have affected evidence on variant
  transmissibility and severity
:::
::: {.column width="50%"}
*Incidence and Rt estimates screenshot*
:::
::::

## Analysing data separately: the impact

:::: {.columns}
::: {.column width="50%"}
- Cascade fed into policy-relevant analyses
  of variant transmissibility and severity
- No way to evaluate the impact of inherited
  assumptions on downstream conclusions
- Joint modelling of CIS data was not feasible
  within policy timelines
:::
::: {.column width="50%"}
*Policy paper screenshot*
:::
::::

## Analysing all data together

- Joint modelling avoids cascading bias and
  provides rigorous statistical inference
- But models are difficult to develop within
  policy-relevant timelines
- Require expertise across multiple domains and
  careful specification of component interactions
- PPLs reduce effort but do not provide ways to
  share or reuse model components
- The practical unit of reuse remains the whole
  model or its codebase, not the underlying
  epidemiological concepts

## The Rt estimation ecosystem

- At least seven Stan packages estimate Rt using
  renewal approaches; none share components
- Some packages share authors yet still duplicate
  implementations
- Wastewater Rt tools built by non-wastewater
  experts, share no components, unclear which
  choices matter
- Unit of reuse is the whole model, not the
  underlying epidemiological concepts

## Rt estimation research

- Hundreds of papers propose incremental
  advances to Rt estimation and related methods
- Often not tested against current best practice
  and often built from scratch
- Each represents a year or more of PhD/postdoc
  work for very incremental features
- Features like interaction matrices, day-of-week
  effects, or latent process changes
- Much of this effort is duplicated across groups
  with shared goals
- Little research directly uses the tools
  developed by other groups

## What happens when you try to be modular?

:::: {.columns}
::: {.column width="50%"}
- epinowcast aims for modular Bayesian framework
  to support flexible model specification
- Cannot be separated into reusable parts
- Difficult for users to contribute to or adapt
  beyond supported functionality
- Remains limited in scope despite ambition
:::
::: {.column width="50%"}
*epinowcast screenshot*
:::
::::

## This pattern repeats across domains

- Delay estimation
- Nowcasting
- Viral load modelling
- Severity estimation
- Forecasting and scenario modelling

## The Bayesian workflow

:::: {.columns}
::: {.column width="50%"}
- A systematic workflow guides iterative model
  development, validation, and criticism
- A single specification is needed for both
  simulation and inference
- Following the workflow exposes gaps in
  methodology, inference, and evaluation
- New questions arise on uncertainty propagation,
  interface diagnostics, and data conflicts

github.com/seabbs/a-workflow-for-infectious-disease-modelling
:::
::: {.column width="50%"}
*Workflow figure*
:::
::::

## What is composable modelling?

- Components can be reused across contexts and
  combined in different configurations whilst
  maintaining statistical rigour
- Individual components can be inspected and
  validated in isolation then combined into
  joint models that properly propagate uncertainty
- Domain experts contribute specialised parts
  without understanding the whole system
- Adding data sources requires only a new
  submodel, not rebuilding the entire model
- DSLs in Julia have enabled composable models
  in hydrology and weather forecasting

# Proposed requirements

## Requirements for a composable framework

- Based on challenges above, outbreak modelling
  experience, and Bayesian best practices
- Six themes: uncertainty & inference, model
  structure, population heterogeneity,
  accessibility & adoption, interoperability,
  and modelling workflow
- Themes overlap; e.g. standardised interfaces
  enable component compatibility, multi-strata
  handling, and uncertainty propagation

::: {.callout-warning}
These requirements were developed by the authors
and have not yet received broader community input.
:::

## Requirements: uncertainty & inference

- Probabilistic model specification with full
  uncertainty propagation
- Joint modelling and staged inference with
  proper uncertainty propagation
- Automatic differentiation for efficient
  fitting with modern inference methods
- Switchable inference backends; no single
  approach works universally

## Requirements: model structure

- Clear separation between model components;
  infection, latent, and observation processes
  require distinct expertise
- Easy nesting of models within models; adding
  data sources should not require modifying
  entire models
- Support for multiple modelling paradigms
  including compartmental, agent-based, and
  network models

## Requirements: population heterogeneity

- Arbitrary stratification for all model
  components across age, location, risk group
- Automatic management of multi-group structure;
  managing dimensions and interactions is complex
  and error-prone
- Programmatic generation and modification of
  model components
- Partial pooling to share information across
  groups with sparse data

## Requirements: accessibility & adoption

- Clear, concise modelling language for diverse
  users including those without programming
  expertise
- Components contain both structure and parameter
  prior distributions to encode domain expertise
- Support for incremental adoption; complete
  rebuilds are wasteful and time-consuming
- Components functional as standalone tools
  outside the compositional framework

## Requirements: interoperability & workflow

- Standardised interfaces between components
  for correct uncertainty propagation
- Data-agnostic model definitions that generalise
  across datasets, prior predictive checks, and
  forecasting
- A single specification for simulation and
  inference
- Rapid model iteration; easy validation and
  exploration of model components
- Language optimised for LLM-assisted construction
  with automated validation

# Our approach {subtitle="A front-end DSL for epidemiology with a back-end probabilistic programming language"}

## Our approach

- Domain-specific language for epidemiology
- Modelling backend

## Why a domain-specific language?

- Epidemiological models share common structure:
  infection processes, latent dynamics,
  observation models
- A DSL lets domain experts specify models using
  epidemiological concepts rather than
  programming constructs
- Components encode domain expertise including
  prior distributions, making it transferable
  between models and teams
- Proven in other fields: SpeedyWeather.jl for
  weather, HydroModels.jl for hydrology
- Julia's type system and multiple dispatch
  enable building blocks that compose cleanly
  across model hierarchies

## Domain-specific language

- Abstract types define interfaces;
  struct-in-struct pattern for nesting
  simpler components into complex models
- Three type hierarchies: AbstractEpiModel
  (infection), AbstractLatentModel
  (time-varying parameters),
  AbstractObservationModel (links latent
  to observed)
- Models compose across hierarchies,
  e.g. observation models can contain
  latent processes as fields
- EpiProblem assembles components into a
  complete model; structures are
  data-agnostic and reusable across
  datasets

## Why Turing.jl?

- Requirements need probabilistic reasoning,
  so a probabilistic programming language (PPL)
- Only Julia PPLs provide the metaprogramming
  needed for domain-specific abstractions,
  arbitrary stratification, and programming
  over model structure
- Turing.jl has mature submodel support for
  nesting models within models and extensive
  inference algorithm choice
- Light abstraction layer on top of the wider
  Julia ecosystem rather than a walled garden
- SciML ecosystem for ODEs, neural networks,
  and other scientific computing tools

## Turing.jl as a modelling backend

- generate functions dispatch on abstract
  types; multiple dispatch selects the
  implementation
- Produces Turing.jl code via the @model
  macro; dual purpose for simulation or
  inference
- @submodel macro enables nesting,
  so composed DSL components recurse
  through arbitrary depth
- Many computational components are
  backend-agnostic and portable to other
  frameworks
- Cross-ecosystem access via EpiAwareR,
  an R interface using JuliaCall

## Composable applications

![](../../figures/fig-composable.png)

# Case studies

## Autoregressive example: model

:::: {.columns}
::: {.column width="50%"}
- AR(2) defined with the `AR` struct using
  priors from `Distributions.jl`
- ARMA(2,1) composed by setting the AR
  error term to an `MA` model
- ARIMA(2,1,1) composed by wrapping with
  `DiffLatentModel` for differencing
- Combined with log-Poisson observation
  for a complete generative model
:::
::: {.column width="50%"}
![](../../figures/fig-case-studies.png)
:::
::::

## Autoregressive example: results

:::: {.columns}
::: {.column width="50%"}
- Posterior distributions recover the
  simulated parameter values
- Prior predictive samples show expected
  behaviour for each component
- Same components reused in all subsequent
  case studies
:::
::: {.column width="50%"}
![](../../figures/fig-arima.png)
:::
::::

## Renewal model (Mishra et al.): model

:::: {.columns}
::: {.column width="50%"}
- Estimates time-varying R_t from COVID-19
  cases in South Korea
- Composes AR(2) for log R_t, `Renewal`
  for infections, `NegativeBinomialError`
  for observations
- Reuses the AR(2) component from the
  ARIMA example
:::
::: {.column width="50%"}
![](../../figures/fig-case-studies.png)
:::
::::

## Renewal model (Mishra et al.): results

:::: {.columns}
::: {.column width="50%"}
- Prior predictive checks for each
  component in isolation
- Recovers R_t peaked at ~10 then rapidly
  dropped below 1 in early March 2020
- Posterior predictive distribution captures
  observed case dynamics
:::
::: {.column width="50%"}
![](../../figures/fig-mishra.png)
:::
::::

## Real-time nowcasting (EpiNow2): model

:::: {.columns}
::: {.column width="50%"}
- Reuses ARIMA(2,1,1) and negative binomial
  from case study 1
- Adds weekly R_t, incubation/reporting
  delay convolutions, day-of-week effects
- Uses generation time distribution rather
  than serial interval
:::
::: {.column width="50%"}
![](../../figures/fig-case-studies.png)
:::
::::

## Real-time nowcasting (EpiNow2): results

:::: {.columns}
::: {.column width="50%"}
- Widely used real-time tool replicated
  through composition of DSL components
- Also replicated via `EpiAwareR` R
  interface
- Posterior accounts for reporting delays
  whilst recovering R_t dynamics
:::
::: {.column width="50%"}
![](../../figures/fig-epinow2.png)
:::
::::

## ODE model (influenza): model

:::: {.columns}
::: {.column width="50%"}
- SIR compartmental model as alternative
  infection process via `ODEProcess`
- Two variants: deterministic Poisson and
  stochastic AR(1) ascertainment
- Same observation and latent components
  work with fundamentally different
  infection processes
:::
::: {.column width="50%"}
![](../../figures/fig-case-studies.png)
:::
::::

## ODE model (influenza): results

:::: {.columns}
::: {.column width="50%"}
- Both models recover R_0 of approx. 2,
  consistent with Chatzilena et al.
- Stochastic model better calibrated;
  deterministic has points outside 95% CIs
- Demonstrates support for multiple
  modelling paradigms from the same
  components
:::
::: {.column width="50%"}
![](../../figures/fig-sir.png)
:::
::::

# Wrap up

## Summary

- Composable models address the tension between
  statistical rigour and flexibility for
  collaborative, timely evidence
- Modular design enables faster, less error prone
  model development and component reuse
- Domain experts contribute specialised components
  without needing to implement other components
- Common components enable attribution of results
  to modelling assumptions, not implementation
- Component structure is well suited for LLM
  assisted model construction

## Alternative approaches

- Category theory / AlgebraicJulia provide formal
  compositional guarantees but limited support
  for probabilistic modelling
- Stan optimised for complete models; Gen.jl
  lower-level; NumPyro/JAX promising but
  barriers for epidemiological modellers
- Declarative graph-based PPLs (JuliaBUGS,
  RxInfer) trade flexibility for structure
- Symbolic-numeric frameworks (ModelingToolkit.jl)
  offer automated optimisation
- Agent-based approaches (Starsim, EpiABM) show
  promise but calibration remains challenging

## Future work

- Gather community feedback on requirements;
  expand component library across scales
  and data types
- Integrate composable approach with proposed
  modelling workflow for structured iterative
  development and validation
- Explore other composability methods such as
  AlgebraicJulia for formal compositional
  guarantees and symbolic-numeric approaches
- Develop modular Bayesian inference methods
  including Markov melding and cut likelihoods
- Improve Turing.jl composability: submodel
  handling, conditioning, and post-processing
- Design surveillance programmes with composable
  frameworks in mind for reuse from data
  collection through to policy analyses

## Conclusions

- Composable modelling maintains statistical
  rigour whilst enabling rapid collaborative
  model development
- Domain experts can contribute specialised
  components without understanding entire
  modelling frameworks
- Likely key for enabling robust LLM assisted
  model construction with reduced errors
- Work remains to realise these potential benefits
- Investment in adaptable modelling infrastructure
  for timely evidence for policy is critical

## TL;DR

- Modelling evidence must be timely, rigorous, and
  collaborative but current approaches force a trade-off
- Chaining models offers flexibility but loses information
  and introduces bias; joint models are rigorous but
  monolithic and not reusable
- Composable models can be developed and validated as parts
  then composed for joint analysis, supporting a rigorous
  modelling workflow
- A proof of concept in Julia/R replicates three published
  analyses by composing shared and novel components
- Work remains but investment in adaptable modelling
  infrastructure that can incorporate diverse data to
  provide timely evidence for policy is critical

::: {.callout-note}
## Links

- epiaware.org/ComposableProbabilisticIDModels
:::
